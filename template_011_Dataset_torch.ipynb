{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUA64IUYOhHl"
   },
   "source": [
    "# 011. PyTorch Dataset\n",
    "\n",
    "### Dataset 클래스\n",
    "\n",
    " - PyTorch는 두 개의 데이터 관련 클라스를 제공하여 pre-loaded datasets 및 custom dataset 을 사용할 수 있도록 한다.\n",
    " \n",
    "     - torch.utils.data.Dataset - 샘플 및 해당 레이블을 제공\n",
    "     - torch.utils.data.DataLoader - 샘플에 쉽게 액세스 할 수 있도록 Dataset의 iterable 을 wrapping\n",
    "     \n",
    "\n",
    "- PyTorch domain library (Image, Text, Audio dataset) 들은 torch.utils.data.Dataset 을 상속 받은 pre-loaded dataset (ex. FashionMNIST)과 관련 함수 제공\n",
    " \n",
    "\n",
    "- torch.utils.data.Dataset 은 데이터셋을 나타내는 추상클래스이다.\n",
    "\n",
    "\n",
    "- custom 데이터셋은 Dataset 을 상속하고 아래와 같이 Dataset method 를 오버라이드 하여 작성\n",
    "\n",
    "\n",
    "    - 생성자 __init__ 은 dataset 의 전처리를 해주는 부분\n",
    "    - len(dataset) 에서 호출되는 __len__ 은 데이터셋의 크기를 리턴\n",
    "    - dataset[i] 에서 호출되는 __getitem__ 은 𝑖 번째 샘플을 찾는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImddWZXJOhHw"
   },
   "source": [
    "## 사용자 정의 Dataset 작성\n",
    "\n",
    "- 특정 길이의 data를 생성하는 사용자 정의 Dataset class  \n",
    "- transform object를 전달 받으면 data 변환을 하여 반환  \n",
    "- iterable형태로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_set(Dataset):\n",
    "    def __init__(self, length=10, transform=None):\n",
    "        torch.manual_seed(101)\n",
    "        self.x = 10 * torch.ones(length, 2)\n",
    "        self.y = torch.ones(length, 1)\n",
    "        self.len = length\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.x[idx], self.y[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmFtIvrxOhHy"
   },
   "source": [
    "### iterable 형태로 사용\n",
    "- len(dataset)  \n",
    "- dataset[ i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = toy_set()\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    x, y = test_data[i]\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n",
      "tensor([10., 10.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_data:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10., 10.]), tensor([1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29t0psPgOhH1"
   },
   "source": [
    "## Transform 적용\n",
    "\n",
    "- 사용자 정의 transform module 을 Custom Dataset 에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5B50dQMOhH2"
   },
   "source": [
    "### Transform 함수 적용 예 : scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(sample):\n",
    "    x, y = sample\n",
    "    scaled_x = x / 100000\n",
    "    scaled_y = y / 10\n",
    "    return scaled_x, scaled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000e-04, 1.0000e-04]), tensor([0.1000]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = toy_set(3, scaling)\n",
    "\n",
    "dataset_[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlxGcH6hOhH3"
   },
   "source": [
    "### transform class 적용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_ones:\n",
    "    def __init__(self, added=1):\n",
    "        self.added = added\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x = x + self.added\n",
    "        y = y + self.added\n",
    "        sample = x, y\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11., 11.]), tensor([2.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_m = add_ones()\n",
    "\n",
    "dataset_ = toy_set(10, transform=a_m)\n",
    "dataset_[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFisAyhnOhH5"
   },
   "source": [
    "### Transform 을 동시에 여러개 적용 : transform.Compose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "data_transforms = transforms.Compose([scaling, a_m])\n",
    "\n",
    "data2 = toy_set(5, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0001, 1.0001]) tensor([1.1000])\n",
      "tensor([1.0001, 1.0001]) tensor([1.1000])\n",
      "tensor([1.0001, 1.0001]) tensor([1.1000])\n",
      "tensor([1.0001, 1.0001]) tensor([1.1000])\n",
      "tensor([1.0001, 1.0001]) tensor([1.1000])\n"
     ]
    }
   ],
   "source": [
    "#data2 = toy_set(5)\n",
    "for x, y in data2:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWID8_6yOhH7"
   },
   "source": [
    "<h1>pre-built Datasets and Transforms</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_tensor_transform = transforms.Compose(\n",
    "        [transforms.CenterCrop(20),\n",
    "        transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=crop_tensor_transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train=False,\n",
    "    transform=crop_tensor_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 20])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_data[0][0].reshape(20, 20), cmap=\"gray\")\n",
    "plt.title(\"y = {}\".format(training_data[0][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HAG7FNTOhH-"
   },
   "source": [
    "## DataLoader를 사용하여 training 데이터 준비\n",
    "\n",
    "Dataset은 `한 번에 한 개씩 샘플`의 feature 와 label 을 retreive 합니다. 모델을 훈련하는 동안 일반적으로 `minibatch`로 샘플을 전달하고, 매 epoch 마다 데이터를 reshuffle 하여 overfitting을 줄이며, Python의 multiprocessing을 사용하여 읽는 속도를 높입니다.\n",
    "\n",
    "DataLoader는 쉬운 API로 이러한 복잡성 내용을 추상화한 반복자(iterable) 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLacMc-IOhH_"
   },
   "source": [
    "## DataLoader를 통해 반복\n",
    "해당 데이터 세트를 ``Dataloader``에 로드 했으며 반복할 수 있습니다. 아래의 각 반복은`` train_features`` 및 ``train_labels`` ( batch_size=64 의 feature 및 label) 의 배치를 반환합니다.  ``shuffle=True``를 지정했기 때문에 모든 배치를 반복한 후에 데이터가 섞입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 20, 20]), torch.Size([64]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features.size(), train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b39b44b070>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_features[0].squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12wfH6alOhH_"
   },
   "source": [
    "## TensorDataset\n",
    "- PyTorch의 TensorDataset은 tensor를 감싸는 Dataset입니다.\n",
    "- TensorDataset은 Dataset을 상속한 클래스로 학습 데이터 X와 레이블 Y를 묶어 놓는 컨테이너입니다.\n",
    "- TensorDataset을 DataLoader에 전달하면 for 루프에서 데이터의 일부분만 간단히 추출할 수 있게 됩니다.  \n",
    "- TensorDataset에는 텐서만 전달할 수 있으며, Variable은 전달할 수 없으니 주의\n",
    "- Dataset은 직접 작성할 수도 있어서 대량의 이미지 파일을 한 번에 메모리에 저장하지 않고 필요할 때마다 읽어서 학습하는 등 다양하게 활용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7620,  1.3676, -0.8925, -0.6012],\n",
       "         [-0.1406,  0.4350,  0.4254,  0.2158],\n",
       "         [ 0.3394, -0.3691,  1.1529,  0.4924],\n",
       "         [-0.3265, -1.1704, -0.8616,  0.4852],\n",
       "         [-1.2183, -0.2174, -0.4333,  0.7275]], dtype=torch.float64),\n",
       " tensor([0, 1, 1, 0, 0], dtype=torch.int32))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "x = np.random.randn(5, 4)\n",
    "y = np.random.randint(0, 2, size=5)\n",
    "\n",
    "X_train = torch.from_numpy(x)\n",
    "y_train = torch.from_numpy(y)\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7620,  1.3676, -0.8925, -0.6012],\n",
       "         [-0.1406,  0.4350,  0.4254,  0.2158]], dtype=torch.float64),\n",
       " tensor([0, 1], dtype=torch.int32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=2, shuffle=False)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "011_Dataset_torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
